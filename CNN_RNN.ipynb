{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# User Parameters"
      ],
      "metadata": {
        "id": "Op1QDkYCUnNw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Host = \"colab\" #@param [\"colab\", \"AWS\", \"GCP\"]\n",
        "\n",
        "Account = \"colab_datapirates\" #@param[\"colab_datapirates\", \"colab_lahiru_cse\", \"colab_lahiru_personal\"]\n",
        "EMBEDDING_SIZE = 300 #@param [50, 150, 200, 250, 300, 350, 400, 450, 500]\n",
        "embedding_type = \"fasttext\" #@param [\"fasttext\",\"word2vec\"]\n",
        "experiment_no = \"1001\" #@param [] {allow-input: true}\n",
        "model_type = \"LSTM\" #@param [\"RNN\",\"GRU\", \"LSTM\", \"BiLSTM\" ] \n",
        "\n",
        "stack_modeles = \"\" #@param [\"\",\"2\",\"3\"]\n",
        "apply_CNN = False #@param {type:\"boolean\"}\n",
        "\n",
        "model_name = model_type + \"_model\"\n",
        "if(stack_modeles == \"2\" or stack_modeles == \"3\"):\n",
        "  model_name = \"stacked_\" + model_name + \"_\" + stack_modeles\n",
        "if(apply_CNN):\n",
        "  model_name = \"CNN_\" + model_name \n",
        "\n",
        "print(model_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZxEJJROyUswe",
        "outputId": "7df7105f-f071-4487-86ad-b7cc00b873cb"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LSTM_model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Folder Paths"
      ],
      "metadata": {
        "id": "Oo3GjhT3VA6C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "folder_path = '/content/drive/MyDrive/'\n",
        "\n",
        "edp = folder_path + 'Sinhala review dataset form/DataSet/FinalDataSet/CommentsLB3columnDocid  .csv'\n",
        "gdp = folder_path + 'Sinhala review dataset form/DataSet/FinalDataSet/CommentsLB3columnDocid  .csv'\n",
        "\n",
        "context = 5\n",
        "word_embedding_path = folder_path + \"word_embedding/\"+embedding_type+\"/source2_data_from_ecommerce_and_googleforem/\"+str(EMBEDDING_SIZE)+\"/\"+embedding_type+\"_\"+str(EMBEDDING_SIZE)+\"_\"+str(context)\n",
        "word_embedding_keydvectors_path = folder_path + \"word_embedding/\"+embedding_type+\"/source2_data_from_ecommerce_and_googleforem/\"+str(EMBEDDING_SIZE)+\"/keyed_vectors/keyed.kv\"\n",
        "embedding_matrix_path = folder_path + \"Sentiment_Analysis/CNN_RNN/embedding_matrix/\"+embedding_type+\"ecommerce_googleforem\"+str(EMBEDDING_SIZE)+\"_\"+str(context)\n",
        "\n",
        "experiment_name = folder_path + \"Sentiment Analysis/CNN RNN/experiments/\" +str(experiment_no) + \"_\"+ model_name +\"_\"+embedding_type+\"_\"+str(EMBEDDING_SIZE)+\"_\"+str(context)\n",
        "model_save_path = folder_path + \"Sentiment Analysis/CNN RNN/saved_models/\"+str(experiment_no)+\"_weights_best_\"+model_name+\"_\"+embedding_type+\"_\"+str(experiment_no)+\".hdf5\""
      ],
      "metadata": {
        "id": "jylfdY3AU8ru"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dependencies"
      ],
      "metadata": {
        "id": "NhrxxyXkVOrP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow==1.15.2\n",
        "!pip install q keras==2.3.1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pgVJGJLJVRNq",
        "outputId": "6efbc2e4-feac-42e8-ff23-cd708abc965a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow==1.15.2 in /usr/local/lib/python3.7/dist-packages (1.15.2)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (1.1.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (1.47.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (1.21.6)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (1.2.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (1.0.8)\n",
            "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (0.2.2)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (1.1.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (3.3.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (1.14.1)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (0.8.1)\n",
            "Requirement already satisfied: tensorflow-estimator==1.15.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (1.15.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (0.2.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (0.37.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (1.15.0)\n",
            "Requirement already satisfied: tensorboard<1.16.0,>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (1.15.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (3.17.3)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.8->tensorflow==1.15.2) (3.1.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (3.4.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (57.4.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (1.0.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (4.12.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (3.8.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (4.1.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras-applications>=1.0.8->tensorflow==1.15.2) (1.5.2)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: q in /usr/local/lib/python3.7/dist-packages (2.7)\n",
            "Requirement already satisfied: keras==2.3.1 in /usr/local/lib/python3.7/dist-packages (2.3.1)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from keras==2.3.1) (1.21.6)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from keras==2.3.1) (1.15.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from keras==2.3.1) (1.1.2)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.7/dist-packages (from keras==2.3.1) (1.0.8)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from keras==2.3.1) (1.7.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras==2.3.1) (6.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras==2.3.1) (3.1.0)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras==2.3.1) (1.5.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kYQ76qHIVg4I",
        "outputId": "e30d5d2f-dbf6-416a-e0c2-24c94d38b327"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import collections\n",
        "import pickle\n",
        "import re\n",
        "import random\n",
        "import sys\n",
        "import os \n",
        "import time\n",
        "\n",
        "import gensim\n",
        "from gensim.models.keyedvectors import KeyedVectors\n",
        "from gensim.models.fasttext import FastText\n",
        "from gensim.models import word2vec\n",
        "\n",
        "from sklearn.model_selection import train_test_split,cross_val_score, cross_val_predict, KFold, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix,precision_recall_fscore_support\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from numpy import array\n",
        "from numpy import asarray\n",
        "from numpy import zeros\n",
        "from numpy import cumsum\n",
        "\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras import backend as K\n",
        "from keras.models import Sequential,Model,load_model\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Dropout, Activation, Flatten, \\\n",
        "    Embedding, Convolution1D, MaxPooling1D, AveragePooling1D, \\\n",
        "    Input, Dense, merge, Add,TimeDistributed, Bidirectional,SpatialDropout1D\n",
        "from keras.layers.recurrent import LSTM, GRU, SimpleRNN\n",
        "from keras.regularizers import l2, l1_l2\n",
        "from keras.constraints import maxnorm\n",
        "from keras import callbacks\n",
        "from keras.utils import generic_utils,plot_model\n",
        "from keras.optimizers import Adadelta\n",
        "from keras.callbacks import ModelCheckpoint,EarlyStopping\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "\n",
        "import matplotlib.image  as mpimg\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "vqqKubo_3FuB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c78e211f-35c4-40ea-8ab8-2f040737be0d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using TensorFlow backend.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Data"
      ],
      "metadata": {
        "id": "a1WFRdOuW7cT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ecd = pd.read_csv(edp)\n",
        "gfd = pd.read_csv(gdp)"
      ],
      "metadata": {
        "id": "-a4JC7bUW9UW"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ecd.isnull().sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_tiY3Fyc9FXl",
        "outputId": "93367bf7-2e4d-4fff-a1f1-5c43e8aabcc6"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "docid       0\n",
              "Comments    2\n",
              "Lable       0\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ecd['Comments'].isnull().sum()"
      ],
      "metadata": {
        "id": "Ak9bEDVY9IHk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0006465c-6006-4cd0-bb14-5ce940a4832d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "modifiedEcdDataPath = ecd.dropna()"
      ],
      "metadata": {
        "id": "fW8aHhvQ9KMQ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modifiedEcdDataPath.isnull().sum()"
      ],
      "metadata": {
        "id": "mBQOlERN9M-Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "808da553-d493-40f0-ab55-4e2f472ba73e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "docid       0\n",
              "Comments    0\n",
              "Lable       0\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "modifiedEcdDataPath.to_csv('modifiedDataPath.csv',index=False)"
      ],
      "metadata": {
        "id": "7WcuKA2N3nVl"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gfd.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0mz1G_fm3o2u",
        "outputId": "88acf782-99cc-46c0-ba87-1e1e335a50e2"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1230, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gfd.isnull().sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oa0MS0Az3scQ",
        "outputId": "630be1bb-cac9-4049-b9a1-22c40c5d9330"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "docid       0\n",
              "Comments    2\n",
              "Lable       0\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gfd['Comments'].isnull().sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FSvqLxcD3uPz",
        "outputId": "c3d1669f-1c05-4f17-8034-c2324dfb3335"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "modifiedGfdDataPath = gfd.dropna()"
      ],
      "metadata": {
        "id": "IhUJdqby3vrW"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modifiedGfdDataPath.isnull().sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xowYH4cE3xt1",
        "outputId": "bcd16cf8-f3c6-4228-fab0-239c7599454f"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "docid       0\n",
              "Comments    0\n",
              "Lable       0\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "modifiedGfdDataPath.to_csv('modifiedDataPath.csv',index=False)"
      ],
      "metadata": {
        "id": "XXrDVYU53zxs"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ANf02KHs32i5"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_data = pd.concat([modifiedGfdDataPath,modifiedGfdDataPath], ignore_index=True)\n",
        "all_data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MGiNTnIU3XiY",
        "outputId": "e86fcfb5-e77d-4c30-ebf7-4085359eb996"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2456, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create Input"
      ],
      "metadata": {
        "id": "J948bEoa5SAy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "comment-label split"
      ],
      "metadata": {
        "id": "F6DCiE3r5Ugu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# edit this later \n",
        "def text_preprocessing(train_data,test_data):\n",
        "  train_data_texts = train_data['Comments']\n",
        "  train_data_labels = train_data['Lable']\n",
        "  test_data_texts = test_data['Comments']\n",
        "  test_data_labels = test_data['Lable']\n",
        "\n",
        "\n",
        "  comment_texts = []\n",
        "  comment_labels = []\n",
        "\n",
        "  train_text = []\n",
        "  test_text = []\n",
        "  train_labels=[]\n",
        "  test_labels=[]\n",
        "\n",
        "  for label in train_data_labels:\n",
        "    if label == \"POSITIVE\":\n",
        "      train_labels.append(1)\n",
        "    else:\n",
        "      train_labels.append(0)\n",
        "  comment_labels.append(train_labels)\n",
        "\n",
        "  for label in test_data_labels:\n",
        "    if label == \"POSITIVE\":\n",
        "      test_labels.append(1)\n",
        "    else:\n",
        "      test_labels.append(0)\n",
        "  comment_labels.append(test_labels)\n",
        "  \n",
        "\n",
        "  for comment in train_data_texts:\n",
        "    lines = []\n",
        "    try:\n",
        "      words = comment.split()\n",
        "      lines += words\n",
        "    except:\n",
        "      continue\n",
        "    train_text.append(lines)\n",
        "  comment_texts.append(train_text)\n",
        "\n",
        "  for comment in test_data_texts:\n",
        "    lines = []\n",
        "    try:\n",
        "      words = comment.split()\n",
        "      lines += words\n",
        "    except:\n",
        "      continue\n",
        "    test_text.append(lines)\n",
        "  comment_texts.append(test_text)\n",
        "\n",
        "\n",
        "  return comment_texts,comment_labels\n",
        "\n",
        "# edit this later \n",
        "def text_preprocessing_1(data):\n",
        "  comments = data['Comments']\n",
        "  labels = data['Lable']\n",
        "\n",
        "  comments_splitted = []\n",
        "  labels_encoded = []\n",
        "\n",
        "  for label in labels:\n",
        "    if label == \"POSITIVE\":\n",
        "      labels_encoded.append(1)\n",
        "    else:\n",
        "      labels_encoded.append(0)\n",
        "\n",
        "  for comment in comments:\n",
        "    lines = []\n",
        "    try:\n",
        "      words = comment.split()\n",
        "      lines += words\n",
        "    except:\n",
        "      continue\n",
        "    comments_splitted.append(lines)\n",
        "  return comments_splitted,labels_encoded\n",
        "\n",
        "\n",
        "def text_preprocessing_2(data):\n",
        "  comments = data['Comments']\n",
        "  labels = data['Lable']\n",
        "\n",
        "  comments_splitted = []\n",
        "\n",
        "  for comment in comments:\n",
        "    lines = []\n",
        "    try:\n",
        "      words = comment.split()\n",
        "      lines += words\n",
        "    except:\n",
        "      continue\n",
        "    comments_splitted.append(lines)\n",
        "\n",
        "  return comments_splitted,labels"
      ],
      "metadata": {
        "id": "m4i28k6K5WLb"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Takenize and Split Data"
      ],
      "metadata": {
        "id": "MkyY0lbd5a_O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "comment_texts, comment_labels = text_preprocessing_2(all_data)\n",
        "\n",
        "# prepare tokenizer\n",
        "\n",
        "t = Tokenizer()\n",
        "t.fit_on_texts(comment_texts)\n",
        "vocab_size = len(t.word_index) + 1\n",
        "print(vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dc7xBMil5aKL",
        "outputId": "7fe22bdd-21f6-4e67-966c-7b5cf4c33761"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3281\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_docs = t.texts_to_sequences(comment_texts)"
      ],
      "metadata": {
        "id": "48QaHv4u7jKE"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_length = len(max(encoded_docs, key=len))\n",
        "padded_docs = pad_sequences(encoded_docs, maxlen=max_length)\n",
        "\n",
        "comment_labels = np.array(comment_labels)\n",
        "padded_docs = np.array(padded_docs)"
      ],
      "metadata": {
        "id": "ZJLRcRq-8CAS"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "comment_labels = pd.get_dummies(comment_labels).values\n",
        "print('Shape of label tensor:', comment_labels.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uVKalyj-8GOm",
        "outputId": "0d9f88b2-408c-4ccb-cf02-d7be877691ca"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of label tensor: (2456, 5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(padded_docs, comment_labels, test_size=0.1, random_state=0)"
      ],
      "metadata": {
        "id": "aI8HBFvW8T94"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(unique, counts) = np.unique(y_test, return_counts = True)\n",
        "frequencies = np.asarray((unique, counts)).T\n",
        "print(frequencies)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l_GDLktb5nA5",
        "outputId": "fff48e53-6455-4b86-b0c4-c6397cb539fb"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[  0 984]\n",
            " [  1 246]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Word Embedding"
      ],
      "metadata": {
        "id": "P9QDVVpC5qvU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Generate Embedding Metrix"
      ],
      "metadata": {
        "id": "lfMfjj045wzi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_embedding_metrix():\n",
        "  if (embedding_type == 'fasttext'):\n",
        "    word_embedding_model = FastText.load(word_embedding_path)\n",
        "  else:\n",
        "    word_embedding_model = word2vec.Word2Vec.load(word_embedding_path)\n",
        "    \n",
        "  word_vectors = word_embedding_model.wv\n",
        "  word_vectors.save(word_embedding_keydvectors_path)\n",
        "  word_vectors = KeyedVectors.load(word_embedding_keydvectors_path, mmap='r')\n",
        "\n",
        "  embeddings_index = dict()\n",
        "  for word, vocab_obj in word_vectors.vocab.items():\n",
        "    embeddings_index[word]=word_vectors[word]\n",
        "\n",
        "  # create a weight matrix for words in training docs\n",
        "  embedding_matrix = zeros((vocab_size, EMBEDDING_SIZE))#embedding_size))\n",
        "  for word, i in t.word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "      embedding_matrix[i] = embedding_vector\n",
        "\n",
        "  pickle.dump(embedding_matrix, open(embedding_matrix_path, 'wb'))\n",
        "  return embedding_matrix\n"
      ],
      "metadata": {
        "id": "6Ncfd41y50dn"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Load Embedding Matrix"
      ],
      "metadata": {
        "id": "O1iqEu4h55gD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pwd\n",
        "\n",
        "def load_word_embedding_atrix():\n",
        "  f = open(embedding_matrix_path, 'rb')\n",
        "  embedding_matrix= np.array(pickle.load(f))\n",
        "  return embedding_matrix"
      ],
      "metadata": {
        "id": "tC4h8BB353wf"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#f = open(embedding_matrix_path, 'rb')"
      ],
      "metadata": {
        "id": "pjAJVolGZwhR"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Models"
      ],
      "metadata": {
        "id": "WS4T5I-C5_dv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "RNN(LSTM/GRU) model"
      ],
      "metadata": {
        "id": "g9NYa0et6GAg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def RNN_model(RNN_layer, maxlen, hidden_dims, l2_reg, drop_out_value_1, drop_out_value_2):\n",
        "    main_input = Input(shape=(maxlen, ), dtype='int32', name='main_input')\n",
        "    embedding  = Embedding(MAX_FEATURES, EMBEDDING_SIZE,\n",
        "                  weights=[EMBEDDING_MATRIX], input_length=maxlen,\n",
        "                  name='embedding' ,trainable=False)(main_input)\n",
        "\n",
        "    embedding = Dropout(drop_out_value_1)(embedding)\n",
        "\n",
        "    x = RNN(hidden_dims)(embedding)\n",
        "\n",
        "    x = Dense(hidden_dims, activation='relu', init='he_normal', \n",
        "              W_constraint = maxnorm(3), b_constraint=maxnorm(3),\n",
        "              name='mlp')(x)\n",
        "\n",
        "    x = Dropout(drop_out_value_2, name='drop')(x)\n",
        "\n",
        "    output = Dense(4, init='he_normal',\n",
        "                   activation='softmax', name='output')(x)\n",
        "\n",
        "    model = Model(input=main_input, output=output ,name=\"RNN_model\")\n",
        "\n",
        "    model.compile(loss={'output':'categorical_crossentropy'},\n",
        "              optimizer=Adadelta(lr=0.95, epsilon=1e-06),\n",
        "              metrics=[\"accuracy\",\n",
        "                       tf.keras.metrics.Precision(),\n",
        "                        tf.keras.metrics.Recall(),\n",
        "                       f1])\n",
        "    \n",
        "    print(model.summary())\n",
        "    return model\n",
        "\n",
        "def stacked_RNN_model_2(RNN_layer, maxlen, hidden_dims, l2_reg, drop_out_value_1, drop_out_value_2):\n",
        "    main_input = Input(shape=(maxlen, ), dtype='int32', name='main_input')\n",
        "    embedding  = Embedding(MAX_FEATURES, EMBEDDING_SIZE,\n",
        "                  weights=[EMBEDDING_MATRIX], input_length=maxlen,\n",
        "                  name='embedding' ,trainable=False)(main_input)\n",
        "\n",
        "    embedding = Dropout(drop_out_value_1)(embedding)\n",
        "\n",
        "    x = RNN_layer(hidden_dims,return_sequences=True)(embedding)\n",
        "    x = RNN_layer(hidden_dims)(x)\n",
        "\n",
        "    x = Dense(hidden_dims, activation='relu', init='he_normal',\n",
        "              W_constraint = maxnorm(3), b_constraint=maxnorm(3),\n",
        "              name='mlp')(x)\n",
        "\n",
        "    x = Dropout(drop_out_value_2, name='drop')(x)\n",
        "\n",
        "    output = Dense(4, init='he_normal',\n",
        "                   activation='softmax', name='output')(x)\n",
        "\n",
        "    model = Model(input=main_input, output=output, name= \"stacked_RNN_model_2\")\n",
        "\n",
        "    model.compile(loss={'output':'categorical_crossentropy'},\n",
        "          optimizer=Adadelta(lr=0.95, epsilon=1e-06),\n",
        "          metrics=[\"accuracy\",\n",
        "                       tf.keras.metrics.Precision(),\n",
        "                        tf.keras.metrics.Recall(),\n",
        "                       f1])\n",
        "\n",
        "    print(model.summary())\n",
        "    return model\n",
        "\n",
        "def stacked_RNN_model_3(RNN_layer, maxlen, hidden_dims, l2_reg, drop_out_value_1, drop_out_value_2):\n",
        "    main_input = Input(shape=(maxlen, ), dtype='int32', name='main_input')\n",
        "    embedding  = Embedding(MAX_FEATURES, EMBEDDING_SIZE,\n",
        "                  weights=[EMBEDDING_MATRIX], input_length=maxlen,\n",
        "                  name='embedding' ,trainable=False)(main_input)\n",
        "\n",
        "    embedding = Dropout(drop_out_value_1)(embedding)\n",
        "\n",
        "    x = RNN_layer(hidden_dims,return_sequences=True)(embedding)\n",
        "    x = RNN_layer(hidden_dims,return_sequences=True)(x)\n",
        "    x = RNN_layer(hidden_dims)(x)\n",
        "\n",
        "    x = Dense(hidden_dims, activation='relu', init='he_normal',\n",
        "              W_constraint = maxnorm(3), b_constraint=maxnorm(3),\n",
        "              name='mlp')(x)\n",
        "\n",
        "    x = Dropout(drop_out_value_2, name='drop')(x)\n",
        "\n",
        "    output = Dense(4, init='he_normal',\n",
        "                   activation='softmax', name='output')(x)\n",
        "\n",
        "    model = Model(input=main_input, output=output, name=\"stacked_RNN_model_3\")\n",
        "\n",
        "    model.compile(loss={'output':'categorical_crossentropy'},\n",
        "      optimizer=Adadelta(lr=0.95, epsilon=1e-06),\n",
        "      metrics=[\"accuracy\",\n",
        "                    tf.keras.metrics.Precision(),\n",
        "                    tf.keras.metrics.Recall(),\n",
        "                    f1])\n",
        "\n",
        "    print(model.summary())\n",
        "    return model"
      ],
      "metadata": {
        "id": "2k8sUqpg6HcC"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "CNN+RNN(LSTM /GRU) model"
      ],
      "metadata": {
        "id": "vVMGPW5T6PlB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def CNN_RNN_model(RNN_layer, maxlen, hidden_dims, l2_reg, drop_out_value_1, drop_out_value_2):\n",
        "    main_input = Input(shape=(maxlen, ), dtype='int32', name='main_input')\n",
        "    embedding  = Embedding(MAX_FEATURES, EMBEDDING_SIZE,\n",
        "                  weights=[EMBEDDING_MATRIX], input_length=maxlen,\n",
        "                  name='embedding' ,trainable=False)(main_input)\n",
        "\n",
        "    embedding = Dropout(drop_out_value_1)(embedding)\n",
        "\n",
        "    conv4 = Convolution1D(NB_FILTERS,\n",
        "                          4,\n",
        "                          border_mode='valid',\n",
        "                          activation='relu',\n",
        "                          subsample_length=1,\n",
        "                          name='conv4')(embedding)\n",
        "    maxConv4 = MaxPooling1D(pool_length=2,\n",
        "                             name='maxConv4')(conv4)\n",
        "\n",
        "    conv5 = Convolution1D(NB_FILTERS,\n",
        "                          5,\n",
        "                          border_mode='valid',\n",
        "                          activation='relu',\n",
        "                          subsample_length=1,\n",
        "                          name='conv5')(embedding)\n",
        "    maxConv5 = MaxPooling1D(pool_length=2,\n",
        "                            name='maxConv5')(conv5)\n",
        "\n",
        "    x = keras.layers.Concatenate(axis=1)([maxConv4, maxConv5])\n",
        "\n",
        "    x = Dropout(drop_out_value_2)(x)\n",
        "\n",
        "    x = RNN(rnn_output_size)(x)\n",
        "\n",
        "\n",
        "    x = Dense(hidden_dims, activation='relu', init='he_normal',\n",
        "              W_constraint = maxnorm(3), b_constraint=maxnorm(3),\n",
        "              name='mlp')(x)\n",
        "\n",
        "    x = Dropout(drop_out_value_2, name='drop')(x)\n",
        "\n",
        "    output = Dense(4, init='he_normal',\n",
        "                   activation='softmax', name='output')(x)\n",
        "\n",
        "    model = Model(input=main_input, output=output, name= \"CNN+RNN model\")\n",
        "\n",
        "    model.compile(loss={'output':'categorical_crossentropy'},\n",
        "      optimizer=Adadelta(lr=0.95, epsilon=1e-06),\n",
        "      metrics=[\"accuracy\",\n",
        "                    tf.keras.metrics.Precision(),\n",
        "                    tf.keras.metrics.Recall(),\n",
        "                    f1])\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "81Y7GQvL6OCR"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#Train and Evaluate Model"
      ],
      "metadata": {
        "id": "DqCpLhj36Y0U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Custom F1 Implementation"
      ],
      "metadata": {
        "id": "J1_Lrm2m6bwc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def f1(y_true, y_pred):\n",
        "    def recall(y_true, y_pred):\n",
        "        \"\"\"Recall metric.\n",
        "\n",
        "        Only computes a batch-wise average of recall.\n",
        "\n",
        "        Computes the recall, a metric for multi-label classification of\n",
        "        how many relevant items are selected.\n",
        "        \"\"\"\n",
        "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "        recall = true_positives / (possible_positives + K.epsilon())\n",
        "        return recall\n",
        "\n",
        "    def precision(y_true, y_pred):\n",
        "        \"\"\"Precision metric.\n",
        "\n",
        "        Only computes a batch-wise average of precision.\n",
        "\n",
        "        Computes the precision, a metric for multi-label classification of\n",
        "        how many selected items are relevant.\n",
        "        \"\"\"\n",
        "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "        precision = true_positives / (predicted_positives + K.epsilon())\n",
        "        return precision\n",
        "    precision = precision(y_true, y_pred)\n",
        "    recall = recall(y_true, y_pred)\n",
        "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
      ],
      "metadata": {
        "id": "NytOi9NR6ltg"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Train and Validate model\n"
      ],
      "metadata": {
        "id": "AOoeZ2a46qe6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Train_Model_old(model,X_train, X_test, y_train, y_test):\n",
        "\n",
        "  print('Training and Testing...')\n",
        "  test_accs = []\n",
        "  first_run = True\n",
        "\n",
        "\n",
        "  acc=[]\n",
        "  val_acc=[]\n",
        "  loss=[]\n",
        "  val_loss=[]\n",
        "  best_val_acc = 0\n",
        "  best_test_acc = 0\n",
        "  for j in range(nb_epoch):\n",
        "      a = time.time()\n",
        "      his = model.fit(X_train, y_train,\n",
        "                      batch_size=batch_size,\n",
        "                      validation_data=[X_test, y_test],\n",
        "                      shuffle=True,\n",
        "                      epochs=1, verbose=verbosity)\n",
        "      acc+=his.history['accuracy']\n",
        "      val_acc+=his.history['val_accuracy']\n",
        "      loss+=his.history['loss']\n",
        "      val_loss+=his.history['val_loss']\n",
        "      # print('Epoch %d/%d\\t%s' % (j + 1, nb_epoch, str(his.history)))\n",
        "      if his.history['val_accuracy'][0] >= best_val_acc:\n",
        "          score, test_acc = model.evaluate(X_test, y_test,\n",
        "                                      batch_size=batch_size,\n",
        "                                      verbose=2)\n",
        "          best_val_acc = his.history['val_accuracy'][0]\n",
        "          best_test_acc = test_acc\n",
        "          print('Got best epoch  best val acc is %f test acc is %f' %\n",
        "                (best_val_acc, best_test_acc))\n",
        "          if len(test_accs) > 0:\n",
        "              print('Current avg test acc:', str(np.mean(test_accs)))\n",
        "      b = time.time()\n",
        "      cost = b - a\n",
        "      left = (nb_epoch - j - 1)\n",
        "      print('One round cost %ds, %d round %ds %dmin left' % (cost, left,\n",
        "                                                            cost * left,\n",
        "                                                            cost * left / 60.0))\n",
        "      test_accs.append(best_test_acc)\n",
        "\n",
        "  print('Avg test acc:', str(np.mean(test_accs)))\n",
        "  return model\n"
      ],
      "metadata": {
        "id": "vBQdKv4o6uwZ"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Train_Model(model,X_train, y_train, cross_validation = False):\n",
        "\n",
        "  print('Training and Testing...')\n",
        "  \n",
        "  es = EarlyStopping(monitor='val_f1', mode='max', verbose=1, patience=5)\n",
        "  checkpoint = ModelCheckpoint(model_save_path, monitor='val_f1', verbose=1, save_best_only=True, mode='max')\n",
        "  callbacks_list = [checkpoint,es]\n",
        "\n",
        "  if (cross_validation):\n",
        "    callbacks_list = [es]\n",
        "\n",
        "  his = model.fit(X_train, y_train, validation_split=VALIDATION_SPLIT, epochs=NB_EPOCHS, batch_size=BATCH_SIZE, callbacks=callbacks_list, verbose=1)\n",
        "  return model,his"
      ],
      "metadata": {
        "id": "jVs8YKaS6y3c"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Cross Validation"
      ],
      "metadata": {
        "id": "Skl2iYRE61oZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Do_Cross_Validation(X,y):\n",
        "\n",
        "  # Define per-fold score containers\n",
        "  loss_per_fold = []\n",
        "  acc_per_fold = []\n",
        "  precision_per_fold = []\n",
        "  recall_per_fold = []\n",
        "  f1_per_fold = []\n",
        "  \n",
        "\n",
        "  kfold = KFold(n_splits=FOLDS, shuffle=True)\n",
        "\n",
        "  fold_no = 1\n",
        "  inputs = X\n",
        "  targets = y\n",
        "  for train, test in kfold.split(inputs, targets):\n",
        "\n",
        "    # model = build_model()\n",
        "    model = RNN_model(GRU, MAX_LEN, HIDDEN_DIMS, L2_REG, DROPOUT_VALUE_1, DROPOUT_VALUE_2)\n",
        "    \n",
        "    # RNN_layer, maxlen, hidden_dims, l2_reg, drop_out_value_1, drop_out_value_2\n",
        "\n",
        "    # Generate a print\n",
        "    print('------------------------------------------------------------------------')\n",
        "    print(f'Training for fold {fold_no} ...')\n",
        "\n",
        "    # Fit data to model\n",
        "    model, his = Train_Model(model,inputs[train], targets[train], cross_validation=True)\n",
        " \n",
        "    # Generate generalization metrics\n",
        "    scores = model.evaluate(inputs[test], targets[test], verbose=0)\n",
        "\n",
        "    print(f\"\"\"Score for fold {fold_no}:\n",
        "     {model.metrics_names[0]} of {scores[0]}; \n",
        "     {model.metrics_names[1]} of {scores[1]*100}% ;\n",
        "     {model.metrics_names[2]} of {scores[2]*100}% ;\n",
        "     {model.metrics_names[3]} of {scores[3]*100}% ;\n",
        "     {model.metrics_names[4]} of {scores[4]*100}% ;\n",
        "     \"\"\")\n",
        "    \n",
        "    loss_per_fold.append(scores[0])\n",
        "    acc_per_fold.append(scores[1])\n",
        "    precision_per_fold.append(scores[2])\n",
        "    recall_per_fold.append(scores[3])\n",
        "    f1_per_fold.append(scores[4])\n",
        "\n",
        "    # Increase fold number\n",
        "    fold_no = fold_no + 1\n",
        "\n",
        "  # == Provide average scores ==\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print('Score per fold')\n",
        "  for i in range(0, len(acc_per_fold)):\n",
        "    print('------------------------------------------------------------------------')\n",
        "    print(f\"\"\"> Fold {i+1} - \n",
        "    Loss: {loss_per_fold[i]} - \n",
        "    Accuracy: {acc_per_fold[i]}% - \n",
        "    Precesion: {precision_per_fold[i]}% - \n",
        "    Recall: {recall_per_fold[i]}% - \n",
        "    F1: {f1_per_fold[i]}%\n",
        "    \"\"\")\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print('Average scores for all folds:')\n",
        "  print(f'> Loss: {np.mean(loss_per_fold)}')\n",
        "  print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
        "  print(f'> Precision: {np.mean(precision_per_fold)}')\n",
        "  print(f'> Recall: {np.mean(recall_per_fold)}')\n",
        "  print(f'> F1: {np.mean(f1_per_fold)}')\n",
        "  print('------------------------------------------------------------------------')"
      ],
      "metadata": {
        "id": "jK2eUn2H66ol"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#Plot Graphs\n"
      ],
      "metadata": {
        "id": "k_xhQt-R7Ai-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Plot_graphs(metric,val_metric,metric_name):\n",
        "\n",
        "  epochs=range(len(metric)) # Get number of epochs\n",
        "\n",
        "  if metric_name == \"accuracy\":\n",
        "    #------------------------------------------------\n",
        "    # Plot training and validation accuracy per epoch\n",
        "    #------------------------------------------------\n",
        "    plt.plot(epochs, metric, 'r')\n",
        "    plt.plot(epochs, val_metric, 'b')\n",
        "    plt.title('Training and validation accuracy')\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "    plt.legend([\"Accuracy\", \"Validation Accuracy\"])\n",
        "\n",
        "    plt.figure()\n",
        "\n",
        "  elif metric_name == \"loss\" :\n",
        "    #------------------------------------------------\n",
        "    # Plot training and validation loss per epoch\n",
        "    #------------------------------------------------\n",
        "    plt.plot(epochs, metric, 'r')\n",
        "    plt.plot(epochs, val_metric, 'b')\n",
        "    plt.title('Training and validation loss')\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.legend([\"Loss\", \"Validation Loss\"])\n",
        "\n",
        "    plt.figure()\n",
        "\n",
        "  elif metric_name == \"f1\" :\n",
        "    #------------------------------------------------\n",
        "    # Plot training and validation loss per epoch\n",
        "    #------------------------------------------------\n",
        "    plt.plot(epochs, metric, 'r')\n",
        "    plt.plot(epochs, val_metric, 'b')\n",
        "    plt.title('Training and validation F1')\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(\"F1\")\n",
        "    plt.legend([\"F1\", \"Validation F1\"])\n",
        "\n",
        "    plt.figure()\n",
        "\n",
        "\n",
        "  # Expected Output\n",
        "  # A chart where the validation loss does not increase sharply!"
      ],
      "metadata": {
        "id": "lorPPOjg7E7x"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_matrix_path"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "kNkrX8mCcJzk",
        "outputId": "00a2e908-ba00-42ff-888a-7ff3d25d038e"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/Sentiment_Analysis/CNN_RNN/embedding_matrix/fasttextecommerce_googleforem300_5'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main Method\n"
      ],
      "metadata": {
        "id": "yo0ObwFV7LYB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Set Hyper-Parameters\n"
      ],
      "metadata": {
        "id": "YHnSi4qi7QRR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# EMBEDDING_MATRIX = generate_embedding_metrix()\n",
        "%ls\n",
        "%cd ../../\n",
        "%ls\n",
        "EMBEDDING_MATRIX = load_word_embedding_atrix()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 495
        },
        "id": "sC9JHow57Ol4",
        "outputId": "9f908ac1-824b-42a1-a3f0-35c3a30b74ac"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34mbin\u001b[0m/      \u001b[01;34mdev\u001b[0m/   \u001b[01;34mlib32\u001b[0m/  modifiedDataPath.csv      \u001b[01;34mpython-apt\u001b[0m/  \u001b[01;34msrv\u001b[0m/    \u001b[01;34musr\u001b[0m/\n",
            "\u001b[01;34mboot\u001b[0m/     \u001b[01;34metc\u001b[0m/   \u001b[01;34mlib64\u001b[0m/  NGC-DL-CONTAINER-LICENSE  \u001b[01;34mroot\u001b[0m/        \u001b[01;34msys\u001b[0m/    \u001b[01;34mvar\u001b[0m/\n",
            "\u001b[01;34mcontent\u001b[0m/  \u001b[01;34mhome\u001b[0m/  \u001b[01;34mmedia\u001b[0m/  \u001b[01;34mopt\u001b[0m/                      \u001b[01;34mrun\u001b[0m/         \u001b[30;42mtmp\u001b[0m/\n",
            "\u001b[01;34mdatalab\u001b[0m/  \u001b[01;34mlib\u001b[0m/   \u001b[01;34mmnt\u001b[0m/    \u001b[01;34mproc\u001b[0m/                     \u001b[01;34msbin\u001b[0m/        \u001b[01;34mtools\u001b[0m/\n",
            "/\n",
            "\u001b[0m\u001b[01;34mbin\u001b[0m/      \u001b[01;34mdev\u001b[0m/   \u001b[01;34mlib32\u001b[0m/  modifiedDataPath.csv      \u001b[01;34mpython-apt\u001b[0m/  \u001b[01;34msrv\u001b[0m/    \u001b[01;34musr\u001b[0m/\n",
            "\u001b[01;34mboot\u001b[0m/     \u001b[01;34metc\u001b[0m/   \u001b[01;34mlib64\u001b[0m/  NGC-DL-CONTAINER-LICENSE  \u001b[01;34mroot\u001b[0m/        \u001b[01;34msys\u001b[0m/    \u001b[01;34mvar\u001b[0m/\n",
            "\u001b[01;34mcontent\u001b[0m/  \u001b[01;34mhome\u001b[0m/  \u001b[01;34mmedia\u001b[0m/  \u001b[01;34mopt\u001b[0m/                      \u001b[01;34mrun\u001b[0m/         \u001b[30;42mtmp\u001b[0m/\n",
            "\u001b[01;34mdatalab\u001b[0m/  \u001b[01;34mlib\u001b[0m/   \u001b[01;34mmnt\u001b[0m/    \u001b[01;34mproc\u001b[0m/                     \u001b[01;34msbin\u001b[0m/        \u001b[01;34mtools\u001b[0m/\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-3cd981153cb3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cd'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'../../'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ls'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mEMBEDDING_MATRIX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_word_embedding_atrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-27-8d20bb85bb22>\u001b[0m in \u001b[0;36mload_word_embedding_atrix\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_word_embedding_atrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m   \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding_matrix_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m   \u001b[0membedding_matrix\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0membedding_matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/Sentiment_Analysis/CNN_RNN/embedding_matrix/fasttextecommerce_googleforem300_5'"
          ]
        }
      ]
    }
  ]
}