{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PWqEc08xS3NY"
      },
      "source": [
        "# Installing and importing dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0btPiZQv_r0u"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lt9b0WCG_7rf",
        "outputId": "207ad3e0-1926-4e75-d9aa-fb1d0e81778e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow==1.14.0\n",
            "  Downloading tensorflow-1.14.0-cp37-cp37m-manylinux1_x86_64.whl (109.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 109.3 MB 42 kB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (1.1.2)\n",
            "Collecting tensorflow-estimator<1.15.0rc0,>=1.14.0rc0\n",
            "  Downloading tensorflow_estimator-1.14.0-py2.py3-none-any.whl (488 kB)\n",
            "\u001b[K     |████████████████████████████████| 488 kB 45.0 MB/s \n",
            "\u001b[?25hCollecting keras-applications>=1.0.6\n",
            "  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
            "\u001b[K     |████████████████████████████████| 50 kB 5.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (0.8.1)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (3.17.3)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (1.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (1.14.1)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (1.47.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (0.37.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (0.2.0)\n",
            "Collecting tensorboard<1.15.0,>=1.14.0\n",
            "  Downloading tensorboard-1.14.0-py3-none-any.whl (3.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1 MB 41.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (1.2.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (1.1.0)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (0.5.3)\n",
            "Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (1.21.6)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.6->tensorflow==1.14.0) (3.1.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (1.0.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (57.4.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (3.4.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (4.12.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (3.8.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (4.1.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras-applications>=1.0.6->tensorflow==1.14.0) (1.5.2)\n",
            "Installing collected packages: tensorflow-estimator, tensorboard, keras-applications, tensorflow\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.8.0\n",
            "    Uninstalling tensorflow-estimator-2.8.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.8.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.8.0\n",
            "    Uninstalling tensorboard-2.8.0:\n",
            "      Successfully uninstalled tensorboard-2.8.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.8.2+zzzcolab20220719082949\n",
            "    Uninstalling tensorflow-2.8.2+zzzcolab20220719082949:\n",
            "      Successfully uninstalled tensorflow-2.8.2+zzzcolab20220719082949\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "kapre 0.3.7 requires tensorflow>=2.0.0, but you have tensorflow 1.14.0 which is incompatible.\u001b[0m\n",
            "Successfully installed keras-applications-1.0.8 tensorboard-1.14.0 tensorflow-1.14.0 tensorflow-estimator-1.14.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting keras==2.1.5\n",
            "  Downloading Keras-2.1.5-py2.py3-none-any.whl (334 kB)\n",
            "\u001b[K     |████████████████████████████████| 334 kB 14.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from keras==2.1.5) (1.15.0)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from keras==2.1.5) (1.21.6)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from keras==2.1.5) (1.7.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras==2.1.5) (6.0)\n",
            "Installing collected packages: keras\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.8.0\n",
            "    Uninstalling keras-2.8.0:\n",
            "      Successfully uninstalled keras-2.8.0\n",
            "Successfully installed keras-2.1.5\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow==1.14.0 \n",
        "!pip install keras==2.1.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "zZoV3_HJ_-Ur",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb73f102-c0af-4f82-fa92-dc1f662fb6d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using TensorFlow backend.\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import collections\n",
        "import gensim\n",
        "from gensim.models.keyedvectors import KeyedVectors\n",
        "from gensim.models.fasttext import FastText\n",
        "from gensim.models import word2vec\n",
        "\n",
        "from sklearn.model_selection import train_test_split,cross_val_score, cross_val_predict, KFold, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix, precision_recall_fscore_support\n",
        "\n",
        "#import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "#Add My\n",
        "#from pandas import Series\n",
        "#from numpy.random import randn\n",
        "\n",
        "import keras\n",
        "from keras import backend as K\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "import matplotlib.image  as mpimg\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.contrib.layers.python.layers import initializers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "nFGaAWb7BtLm"
      },
      "outputs": [],
      "source": [
        "#set folder paths\n",
        "#folder_path = ''\n",
        "#context = 5\n",
        "\n",
        "#data_path = folder_path + '/content/drive/MyDrive/Sinhala review dataset form/DataSet/FinalDataSet/CommentsLB3columnDocid  .csv'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#set folder paths\n",
        "folder_path = ''\n",
        "EMBEDDING_SIZE = 300 \n",
        "embedding_type = \"fasttext\"\n",
        "context = 5\n",
        "\n",
        "edp = folder_path + '/content/drive/MyDrive/Sinhala review dataset form/DataSet/FinalDataSet/CommentsLB3columnDocid  .csv'\n",
        "gdp = folder_path + '/content/drive/MyDrive/Sinhala review dataset form/DataSet/FinalDataSet/CommentsLB3columnDocid  .csv'\n",
        "\n",
        "word_embedding_path = folder_path + \"word_embedding/\"+embedding_type+\"/source2_data_from_ec_and_gf/\"+str(EMBEDDING_SIZE)+\"/\"+embedding_type+\"_\"+str(EMBEDDING_SIZE)+\"_\"+str(context)\n",
        "word_embedding_keydvectors_path = folder_path + \"word_embedding/\"+embedding_type+\"/source2_data_from_ec_and_gf/\"+str(EMBEDDING_SIZE)+\"/keyed_vectors/keyed.kv\"\n",
        "embedding_matrix_path = folder_path + 'Sentiment Analysis/CNN RNN/embedding_matrix/'+embedding_type+'ec_gf'+str(EMBEDDING_SIZE)+'_'+str(context)"
      ],
      "metadata": {
        "id": "3SlgGtv6ySnx"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#word_embedding_path"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "vHB9OO6JeCY7",
        "outputId": "e414a9bf-c3a6-4ea4-8497-8d67530b0fc5"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'word_embedding/fasttext/source2_data_from_ec_and_gf/300/fasttext_300_5'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQAZtqnKSuay"
      },
      "source": [
        "# Load Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "EteXOPxyCRdX"
      },
      "outputs": [],
      "source": [
        "#dataPath = pd.read_csv(data_path)\n",
        "#dataPath.shape"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#ecd = pd.read_csv(edp, encoding= 'unicode_escape')\n",
        "#gfd = pd.read_csv(gdp, encoding= 'unicode_escape')"
      ],
      "metadata": {
        "id": "ir6AJrnbI-pr"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ecd = pd.read_csv(edp)\n",
        "gfd = pd.read_csv(gdp)"
      ],
      "metadata": {
        "id": "Yb8ptlBpJg7R"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "Sf2znsODmgBy",
        "outputId": "86d17120-95da-43a3-f1a1-d2beeacc96fe"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       docid                                           Comments  Lable\n",
              "0     100204  භාන්ඩය ඉතමත් හොද තත්වයක පවතුනි. නියමිත ප්‍රමිත...      5\n",
              "1     100205                        භාන්ඩය ඉතා හොද තත්වයෙ පවතී.      5\n",
              "2     100206  මෙම වෙබ් අඩවියේ පාරිබෝගික සේවාව ඉතා ඉහළ මට්ටමක...      5\n",
              "3     100207          දුරකථන කවරය ඉතා හොද ප්‍රමිතියකට නිමවා ඇත.      5\n",
              "4     100208  මා බලා පොරොත්තු වූ වෙලද නාමය යටතේම ඇති අයිතම ඉ...      5\n",
              "...      ...                                                ...    ...\n",
              "1225  100445  විස්තරයේ පෙන්වා ඇති පරිදි වේගවත් ආරෝපණයක් ගෙන ...      1\n",
              "1226  100446                 ඒක කකුලටවත් ඇඟටවත් ගැළපෙන්නේ නැහැ.      1\n",
              "1227  100447  භාණ්ඩ කිසි විටෙකත් යවා නැත, මට භාන්ඩය එවීම පිල...      1\n",
              "1228  100448  සායක් ඉතා කෙටි හා ඉතා බාල රෙදි වර්ගයකින් නිමවා...      1\n",
              "1229  100449  මට කිසි දිනක අයිතමය ලැබී නැත, බොහෝ ගැටලු සහ මට...      1\n",
              "\n",
              "[1230 rows x 3 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c4e3720d-ddbb-4a55-9b69-a2516061f7ea\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>docid</th>\n",
              "      <th>Comments</th>\n",
              "      <th>Lable</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>100204</td>\n",
              "      <td>භාන්ඩය ඉතමත් හොද තත්වයක පවතුනි. නියමිත ප්‍රමිත...</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>100205</td>\n",
              "      <td>භාන්ඩය ඉතා හොද තත්වයෙ පවතී.</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>100206</td>\n",
              "      <td>මෙම වෙබ් අඩවියේ පාරිබෝගික සේවාව ඉතා ඉහළ මට්ටමක...</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>100207</td>\n",
              "      <td>දුරකථන කවරය ඉතා හොද ප්‍රමිතියකට නිමවා ඇත.</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>100208</td>\n",
              "      <td>මා බලා පොරොත්තු වූ වෙලද නාමය යටතේම ඇති අයිතම ඉ...</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1225</th>\n",
              "      <td>100445</td>\n",
              "      <td>විස්තරයේ පෙන්වා ඇති පරිදි වේගවත් ආරෝපණයක් ගෙන ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1226</th>\n",
              "      <td>100446</td>\n",
              "      <td>ඒක කකුලටවත් ඇඟටවත් ගැළපෙන්නේ නැහැ.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1227</th>\n",
              "      <td>100447</td>\n",
              "      <td>භාණ්ඩ කිසි විටෙකත් යවා නැත, මට භාන්ඩය එවීම පිල...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1228</th>\n",
              "      <td>100448</td>\n",
              "      <td>සායක් ඉතා කෙටි හා ඉතා බාල රෙදි වර්ගයකින් නිමවා...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1229</th>\n",
              "      <td>100449</td>\n",
              "      <td>මට කිසි දිනක අයිතමය ලැබී නැත, බොහෝ ගැටලු සහ මට...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1230 rows × 3 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c4e3720d-ddbb-4a55-9b69-a2516061f7ea')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c4e3720d-ddbb-4a55-9b69-a2516061f7ea button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c4e3720d-ddbb-4a55-9b69-a2516061f7ea');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "ecd\n",
        "#dataPath"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RzgI4w4tIn8y",
        "outputId": "d1524992-3c7d-4a66-8c5f-cb5bf6d53955"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "docid       0\n",
              "Comments    2\n",
              "Lable       0\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "ecd.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q94e2u8cI-wF",
        "outputId": "14e24aad-8cf5-4ba1-a544-7dcb08c4c5b1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "ecd['Comments'].isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "yVvyk4E7Jh9M"
      },
      "outputs": [],
      "source": [
        "modifiedEcdDataPath = ecd.dropna()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qpkdX_aKJpAd",
        "outputId": "e7da45dc-4134-46ea-a7bf-f3d286f4c276"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "docid       0\n",
              "Comments    0\n",
              "Lable       0\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "modifiedEcdDataPath.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "2gB7r74cJx5-"
      },
      "outputs": [],
      "source": [
        "modifiedEcdDataPath.to_csv('modifiedDataPath.csv',index=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gfd.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H8w9erL2LHNo",
        "outputId": "2ed9cf02-5e23-4913-b156-5397509c4eb7"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1230, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gfd.isnull().sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5dsOOJK9LJ2G",
        "outputId": "c164f4b8-074c-48b0-c93f-94f2a19db311"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "docid       0\n",
              "Comments    2\n",
              "Lable       0\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gfd['Comments'].isnull().sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l7mYlSTMLM3j",
        "outputId": "2b7a8d2d-10e1-49c2-b161-06d695125a04"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "modifiedGfdDataPath = gfd.dropna()"
      ],
      "metadata": {
        "id": "qP2sQ3v9LPCa"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modifiedGfdDataPath.isnull().sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ydQEPp1QLRU1",
        "outputId": "fe4378e7-10de-45c4-a421-6f748677359d"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "docid       0\n",
              "Comments    0\n",
              "Lable       0\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "modifiedGfdDataPath.to_csv('modifiedDataPath.csv',index=False)"
      ],
      "metadata": {
        "id": "RVnk-SkkLTOH"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modifiedGfdDataPath"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "aG0GKfl9PkC2",
        "outputId": "58de8a44-7d04-4f65-fd34-471f9d74e87b"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       docid                                           Comments  Lable\n",
              "0     100204  භාන්ඩය ඉතමත් හොද තත්වයක පවතුනි. නියමිත ප්‍රමිත...      5\n",
              "1     100205                        භාන්ඩය ඉතා හොද තත්වයෙ පවතී.      5\n",
              "2     100206  මෙම වෙබ් අඩවියේ පාරිබෝගික සේවාව ඉතා ඉහළ මට්ටමක...      5\n",
              "3     100207          දුරකථන කවරය ඉතා හොද ප්‍රමිතියකට නිමවා ඇත.      5\n",
              "4     100208  මා බලා පොරොත්තු වූ වෙලද නාමය යටතේම ඇති අයිතම ඉ...      5\n",
              "...      ...                                                ...    ...\n",
              "1225  100445  විස්තරයේ පෙන්වා ඇති පරිදි වේගවත් ආරෝපණයක් ගෙන ...      1\n",
              "1226  100446                 ඒක කකුලටවත් ඇඟටවත් ගැළපෙන්නේ නැහැ.      1\n",
              "1227  100447  භාණ්ඩ කිසි විටෙකත් යවා නැත, මට භාන්ඩය එවීම පිල...      1\n",
              "1228  100448  සායක් ඉතා කෙටි හා ඉතා බාල රෙදි වර්ගයකින් නිමවා...      1\n",
              "1229  100449  මට කිසි දිනක අයිතමය ලැබී නැත, බොහෝ ගැටලු සහ මට...      1\n",
              "\n",
              "[1228 rows x 3 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8ef9e312-736f-4a00-bfdb-903fe07ed08d\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>docid</th>\n",
              "      <th>Comments</th>\n",
              "      <th>Lable</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>100204</td>\n",
              "      <td>භාන්ඩය ඉතමත් හොද තත්වයක පවතුනි. නියමිත ප්‍රමිත...</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>100205</td>\n",
              "      <td>භාන්ඩය ඉතා හොද තත්වයෙ පවතී.</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>100206</td>\n",
              "      <td>මෙම වෙබ් අඩවියේ පාරිබෝගික සේවාව ඉතා ඉහළ මට්ටමක...</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>100207</td>\n",
              "      <td>දුරකථන කවරය ඉතා හොද ප්‍රමිතියකට නිමවා ඇත.</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>100208</td>\n",
              "      <td>මා බලා පොරොත්තු වූ වෙලද නාමය යටතේම ඇති අයිතම ඉ...</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1225</th>\n",
              "      <td>100445</td>\n",
              "      <td>විස්තරයේ පෙන්වා ඇති පරිදි වේගවත් ආරෝපණයක් ගෙන ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1226</th>\n",
              "      <td>100446</td>\n",
              "      <td>ඒක කකුලටවත් ඇඟටවත් ගැළපෙන්නේ නැහැ.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1227</th>\n",
              "      <td>100447</td>\n",
              "      <td>භාණ්ඩ කිසි විටෙකත් යවා නැත, මට භාන්ඩය එවීම පිල...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1228</th>\n",
              "      <td>100448</td>\n",
              "      <td>සායක් ඉතා කෙටි හා ඉතා බාල රෙදි වර්ගයකින් නිමවා...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1229</th>\n",
              "      <td>100449</td>\n",
              "      <td>මට කිසි දිනක අයිතමය ලැබී නැත, බොහෝ ගැටලු සහ මට...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1228 rows × 3 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8ef9e312-736f-4a00-bfdb-903fe07ed08d')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-8ef9e312-736f-4a00-bfdb-903fe07ed08d button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-8ef9e312-736f-4a00-bfdb-903fe07ed08d');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_data = pd.concat([modifiedEcdDataPath,modifiedGfdDataPath], ignore_index=True)\n",
        "all_data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l12WnPHlL-ab",
        "outputId": "a5bf2aa8-11c8-4812-95c6-48ff55f80e20"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2456, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KjqPVsTtm1Q_"
      },
      "source": [
        "# Process data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "4Gj5ZBgnmhtc"
      },
      "outputs": [],
      "source": [
        "def text_preprocessing_2(data):\n",
        "  comments = data['Comments']\n",
        "  labels = data['Lable']\n",
        "  comments_splitted = []\n",
        "\n",
        "  for comment in comments:\n",
        "    lines = []\n",
        "    try:\n",
        "      words = comment.split()\n",
        "      lines += words\n",
        "    except:\n",
        "      continue\n",
        "    comments_splitted.append(lines)\n",
        "\n",
        "  return comments_splitted,labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Em6QukSamEvi",
        "outputId": "9f74b240-1f2a-42db-8da7-2590132c8371"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3316\n"
          ]
        }
      ],
      "source": [
        "comment_texts, comment_labels = text_preprocessing_2(all_data)\n",
        "\n",
        "# prepare tokenizer\n",
        "t = Tokenizer()\n",
        "t.fit_on_texts(comment_texts)\n",
        "vocab_size = len(t.word_index) + 1\n",
        "print(vocab_size)\n",
        "#comment_labels\n",
        "#comment_texts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "Xq3NTbtYmGXK"
      },
      "outputs": [],
      "source": [
        "encoded_docs = t.texts_to_sequences(comment_texts)\n",
        "#encoded_docs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fiuVl2o6KVXU"
      },
      "source": [
        "max_length = len(max(encoded_docs, key=len))\n",
        "padded_docs = pad_sequences(encoded_docs, maxlen=max_length,padding='post')\n",
        "comment_labels = np.array(comment_labels)\n",
        "padded_docs = np.array(padded_docs)\n",
        "comment_labels\n",
        "#padded_docs\n",
        "#max_length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "HPyhjhWnKcWI"
      },
      "outputs": [],
      "source": [
        "max_length = len(max(encoded_docs, key=len))\n",
        "padded_docs = pad_sequences(encoded_docs, maxlen=max_length,padding='post')\n",
        "comment_labels = np.array(comment_labels)\n",
        "padded_docs = np.array(padded_docs)\n",
        "#padded_docs\n",
        "#comment_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "UTOyNPV_KXlX"
      },
      "outputs": [],
      "source": [
        "comment_labels = pd.get_dummies(comment_labels).values\n",
        "#comment_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "HwAQzwWeKZyt"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(padded_docs, comment_labels, test_size=0.1, random_state=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load Embeddings\n",
        "# New section"
      ],
      "metadata": {
        "id": "_nL_WenORT7e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#set embedding path to load embeddings\n",
        "embedding_path = ''"
      ],
      "metadata": {
        "id": "Ay6DABEyRTH3"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_embedding_metrix():\n",
        "  if (embedding_type == 'fasttext'):\n",
        "    word_embedding_model = FastText.load(embedding_path)\n",
        "  else:\n",
        "    word_embedding_model = word2vec.Word2Vec.load(word_embedding_path)\n",
        "    \n",
        "  word_vectors = word_embedding_model.wv\n",
        "  word_vectors.save(word_embedding_keydvectors_path)\n",
        "  word_vectors = KeyedVectors.load(word_embedding_keydvectors_path, mmap='r')\n",
        "\n",
        "  embeddings_index = dict()\n",
        "  for word, vocab_obj in word_vectors.vocab.items():\n",
        "    embeddings_index[word]=word_vectors[word]\n",
        "\n",
        "  # create a weight matrix for words in training docs\n",
        "  embedding_matrix = zeros((vocab_size, embedding_size))\n",
        "  for word, i in t.word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "      embedding_matrix[i] = embedding_vector\n",
        "\n",
        "  pickle.dump(embedding_matrix, open(embedding_matrix_path, 'wb'))\n",
        "  return embedding_matrix"
      ],
      "metadata": {
        "id": "NHDye39bRbIy"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# embedding_vectors = generate_embedding_metrix()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 328
        },
        "id": "UiWygzlTSXAT",
        "outputId": "6db9b0ef-65e7-48b0-8899-22ba3b54b3fc"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-1bdc84b202af>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0membedding_vectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_embedding_metrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-31-b082eb83854c>\u001b[0m in \u001b[0;36mgenerate_embedding_metrix\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgenerate_embedding_metrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0membedding_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'fasttext'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mword_embedding_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFastText\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mword_embedding_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword2vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWord2Vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_embedding_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/models/fasttext.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    934\u001b[0m         \"\"\"\n\u001b[1;32m    935\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 936\u001b[0;31m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFastText\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    937\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainables\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'vectors_vocab_lockf'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'vectors_vocab'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    938\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainables\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectors_vocab_lockf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainables\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mREAL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1243\u001b[0m         \"\"\"\n\u001b[0;32m-> 1244\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseWordEmbeddingsModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1245\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ns_exponent'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1246\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mns_exponent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.75\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, fname_or_handle, **kwargs)\u001b[0m\n\u001b[1;32m    601\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m         \"\"\"\n\u001b[0;32m--> 603\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseAny2VecModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname_or_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    604\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname_or_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/utils.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, fname, mmap)\u001b[0m\n\u001b[1;32m    420\u001b[0m         \u001b[0mcompress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSaveLoad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_adapt_by_suffix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 422\u001b[0;31m         \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    423\u001b[0m         \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_specials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmmap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loaded %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/utils.py\u001b[0m in \u001b[0;36munpickle\u001b[0;34m(fname)\u001b[0m\n\u001b[1;32m   1356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1357\u001b[0m     \"\"\"\n\u001b[0;32m-> 1358\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0msmart_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1359\u001b[0m         \u001b[0;31m# Because of loading from S3 load can't be used (missing readline in smart_open)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1360\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/smart_open/smart_open_lib.py\u001b[0m in \u001b[0;36msmart_open\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    496\u001b[0m     \u001b[0mignore_ext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mignore_extension\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_extension\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 498\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mlocals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/smart_open/smart_open_lib.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(uri, mode, buffering, encoding, errors, newline, closefd, opener, ignore_ext, compression, transport_params)\u001b[0m\n\u001b[1;32m    193\u001b[0m         \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnewline\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m     )\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/smart_open/smart_open_lib.py\u001b[0m in \u001b[0;36m_shortcut_open\u001b[0;34m(uri, mode, compression, buffering, encoding, errors, newline)\u001b[0m\n\u001b[1;32m    359\u001b[0m         \u001b[0mopen_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'errors'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 361\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_builtin_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocal_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffering\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuffering\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mopen_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    362\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: ''"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hPjpsf0BOoCw"
      },
      "source": [
        "# Capsule Network Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "DwmpzyewOnvx"
      },
      "outputs": [],
      "source": [
        "def _conv2d_wrapper(inputs, shape, strides, padding, add_bias, activation_fn, name, stddev=0.1):\n",
        "  with tf.variable_scope(name,reuse=tf.AUTO_REUSE) as scope:\n",
        "    kernel = _get_weights_wrapper(\n",
        "      name='weights', shape=shape, weights_decay_factor=0.0, )\n",
        "    output = tf.nn.conv2d(inputs, filter=kernel, strides=strides, padding=padding, name='conv')\n",
        "    if add_bias:\n",
        "      biases = _get_biases_wrapper(name='biases', shape=[shape[-1]] )\n",
        "      output = tf.add(output, biases, name='biasAdd')\n",
        "    if activation_fn is not None:\n",
        "      output = activation_fn(output, name='activation')\n",
        "  return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "gVBIwseEOxYP"
      },
      "outputs": [],
      "source": [
        "def _get_weights_wrapper(name, shape, dtype=tf.float32, initializer=initializers.xavier_initializer(),weights_decay_factor=None):\n",
        "  weights = _get_variable_wrapper(name=name, shape=shape, dtype=dtype, initializer=initializer)\n",
        "  if weights_decay_factor is not None and weights_decay_factor > 0.0:\n",
        "    weights_wd = tf.multiply(tf.nn.l2_loss(weights), weights_decay_factor, name=name + '/l2loss')\n",
        "    tf.add_to_collection('losses', weights_wd)\n",
        "  return weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "clSDtnzcO0i2"
      },
      "outputs": [],
      "source": [
        "def _get_biases_wrapper(name, shape, dtype=tf.float32, initializer=tf.constant_initializer(0.0)):\n",
        "  \"\"\"Wrapper over _get_variable_wrapper() to get bias.\n",
        "  \"\"\"\n",
        "  biases = _get_variable_wrapper(name=name, shape=shape, dtype=dtype, initializer=initializer)\n",
        "  return biases"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "ENNo--aYO4j2"
      },
      "outputs": [],
      "source": [
        "def _get_variable_wrapper(name, shape=None, dtype=None, initializer=None,regularizer=None,trainable=True,collections=None,caching_device=None,partitioner=None,validate_shape=True,custom_getter=None):\n",
        "  with tf.device('/cpu:0'):\n",
        "    var = tf.get_variable(\n",
        "      name, shape=shape, dtype=dtype, initializer=initializer,\n",
        "      regularizer=regularizer, trainable=trainable,\n",
        "      collections=collections, caching_device=caching_device,\n",
        "      partitioner=partitioner, validate_shape=validate_shape,\n",
        "      custom_getter=custom_getter\n",
        "    )\n",
        "  return var"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "S4bYpI4rO9RQ"
      },
      "outputs": [],
      "source": [
        "def softmax(x, axis=-1):\n",
        "    ex = K.exp(x - K.max(x, axis=axis, keepdims=True))\n",
        "    return ex/K.sum(ex, axis=axis, keepdims=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "ub2baQApO_6f"
      },
      "outputs": [],
      "source": [
        "def squash_v1(x, axis=-1):\n",
        "    s_squared_norm = K.sum(K.square(x), axis, keepdims=True) + K.epsilon()\n",
        "    scale = K.sqrt(s_squared_norm)/ (0.5 + s_squared_norm)\n",
        "    return scale * x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "y8ryflzFPB77"
      },
      "outputs": [],
      "source": [
        "def squash_v0(s, axis=-1, epsilon=1e-7, name=None):\n",
        "    s_squared_norm = K.sum(K.square(s), axis, keepdims=True) + K.epsilon()\n",
        "    safe_norm = K.sqrt(s_squared_norm)\n",
        "    scale = 1 - tf.exp(-safe_norm)\n",
        "    return scale * s / safe_norm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "qj01htMJPGE_"
      },
      "outputs": [],
      "source": [
        "def routing(u_hat_vecs, beta_a, iterations, output_capsule_num, i_activations):\n",
        "    b = keras.backend.zeros_like(u_hat_vecs[:,:,:,0])\n",
        "    if i_activations is not None:\n",
        "        i_activations = i_activations[...,tf.newaxis]\n",
        "    for i in range(iterations):\n",
        "        if False:\n",
        "            leak = tf.zeros_like(b, optimize=True)\n",
        "            leak = tf.reduce_sum(leak, axis=1, keep_dims=True)\n",
        "            leaky_logits = tf.concat([leak, b], axis=1)\n",
        "            leaky_routing = tf.nn.softmax(leaky_logits, dim=1)        \n",
        "            c = tf.split(leaky_routing, [1, output_capsule_num], axis=1)[1]\n",
        "        else:\n",
        "            c = softmax(b, 1)   \n",
        "        outputs = squash_v1(K.batch_dot(c, u_hat_vecs, [2, 2]))\n",
        "        if i < iterations - 1:\n",
        "            b = b + K.batch_dot(outputs, u_hat_vecs, [2, 3])                                    \n",
        "    poses = outputs \n",
        "    activations = K.sqrt(K.sum(K.square(poses), 2))\n",
        "    return poses, activations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "1HNLHaB2PKiW"
      },
      "outputs": [],
      "source": [
        "def vec_transformationByConv(poses, input_capsule_dim, input_capsule_num, output_capsule_dim, output_capsule_num):                            \n",
        "    kernel = _get_weights_wrapper(name='weights', shape=[1, input_capsule_dim, output_capsule_dim*output_capsule_num], weights_decay_factor=0.0)\n",
        "    u_hat_vecs = keras.backend.conv1d(poses, kernel)\n",
        "    u_hat_vecs = keras.backend.reshape(u_hat_vecs, (-1, input_capsule_num, output_capsule_num, output_capsule_dim))\n",
        "    u_hat_vecs = keras.backend.permute_dimensions(u_hat_vecs, (0, 2, 1, 3))\n",
        "    return u_hat_vecs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "WpqQxEn6PNcw"
      },
      "outputs": [],
      "source": [
        "def vec_transformationByMat(poses, input_capsule_dim, input_capsule_num, output_capsule_dim, output_capsule_num, shared=True):                        \n",
        "    inputs_poses_shape = poses.get_shape().as_list()\n",
        "    poses = poses[..., tf.newaxis, :]        \n",
        "    poses = tf.tile(poses, [1, 1, output_capsule_num, 1])    \n",
        "    if shared:\n",
        "        kernel = _get_weights_wrapper(name='weights', shape=[1, 1, output_capsule_num, output_capsule_dim, input_capsule_dim], weights_decay_factor=0.0)\n",
        "        kernel = tf.tile(kernel, [inputs_poses_shape[0], input_capsule_num, 1, 1, 1])\n",
        "    else:\n",
        "        kernel = _get_weights_wrapper(name='weights', shape=[1, input_capsule_num, output_capsule_num, output_capsule_dim, input_capsule_dim], weights_decay_factor=0.0)\n",
        "        kernel = tf.tile(kernel, [inputs_poses_shape[0], 1, 1, 1, 1])\n",
        "    u_hat_vecs = tf.squeeze(tf.matmul(kernel, poses[...,tf.newaxis]),axis=-1)\n",
        "    u_hat_vecs = keras.backend.permute_dimensions(u_hat_vecs, (0, 2, 1, 3))\n",
        "    return u_hat_vecs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "afkYMYK5PSzG"
      },
      "outputs": [],
      "source": [
        "def capsules_init(inputs, shape, strides, padding, pose_shape, add_bias, name):\n",
        "    with tf.variable_scope(name,reuse=tf.AUTO_REUSE):   \n",
        "        poses = _conv2d_wrapper(\n",
        "          inputs,\n",
        "          shape=shape[0:-1] + [shape[-1] * pose_shape],\n",
        "          strides=strides,\n",
        "          padding=padding,\n",
        "          add_bias=add_bias,\n",
        "          activation_fn=None,\n",
        "          name='pose_stacked'\n",
        "        )        \n",
        "        poses_shape = poses.get_shape().as_list()    \n",
        "        poses = tf.reshape(poses, [-1, poses_shape[1], poses_shape[2], shape[-1], pose_shape])        \n",
        "        beta_a = _get_weights_wrapper(name='beta_a', shape=[1, shape[-1]])    \n",
        "        poses = squash_v1(poses, axis=-1)  \n",
        "        activations = K.sqrt(K.sum(K.square(poses), axis=-1)) + beta_a        \n",
        "\n",
        "    return poses, activations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "vTnKzbOFPWhr"
      },
      "outputs": [],
      "source": [
        "def capsule_fc_layer(nets, output_capsule_num, iterations, name):\n",
        "    with tf.variable_scope(name,reuse=tf.AUTO_REUSE):   \n",
        "        poses, i_activations = nets\n",
        "        input_pose_shape = poses.get_shape().as_list()\n",
        "\n",
        "        u_hat_vecs = vec_transformationByConv(poses,input_pose_shape[-1], input_pose_shape[1],input_pose_shape[-1], output_capsule_num,)\n",
        "        beta_a = _get_weights_wrapper(name='beta_a', shape=[1, output_capsule_num])\n",
        "        poses, activations = routing(u_hat_vecs, beta_a, iterations, output_capsule_num, i_activations)\n",
        "    return poses, activations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "F5EQqtiHPa7F"
      },
      "outputs": [],
      "source": [
        "def capsule_flatten(nets):\n",
        "    poses, activations = nets\n",
        "    input_pose_shape = poses.get_shape().as_list()\n",
        "    \n",
        "    poses = tf.reshape(poses, [\n",
        "                    -1, input_pose_shape[1]*input_pose_shape[2]*input_pose_shape[3], input_pose_shape[-1]]) \n",
        "    activations = tf.reshape(activations, [\n",
        "                    -1, input_pose_shape[1]*input_pose_shape[2]*input_pose_shape[3]])\n",
        "    return poses, activations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "sMwy5NQUPidt"
      },
      "outputs": [],
      "source": [
        "def capsule_conv_layer(nets, shape, strides, iterations, name):   \n",
        "    with tf.variable_scope(name,reuse=tf.AUTO_REUSE):              \n",
        "        poses, i_activations = nets\n",
        "        \n",
        "        inputs_poses_shape = poses.get_shape().as_list()\n",
        "\n",
        "        hk_offsets = [\n",
        "          [(h_offset + k_offset) for k_offset in range(0, shape[0])] for h_offset in\n",
        "          range(0, inputs_poses_shape[1] + 1 - shape[0], strides[1])\n",
        "        ]\n",
        "        wk_offsets = [\n",
        "          [(w_offset + k_offset) for k_offset in range(0, shape[1])] for w_offset in\n",
        "          range(0, inputs_poses_shape[2] + 1 - shape[1], strides[2])\n",
        "        ]\n",
        "    \n",
        "        inputs_poses_patches = tf.transpose(\n",
        "          tf.gather(\n",
        "            tf.gather(\n",
        "              poses, hk_offsets, axis=1, name='gather_poses_height_kernel'\n",
        "            ), wk_offsets, axis=3, name='gather_poses_width_kernel'\n",
        "          ), perm=[0, 1, 3, 2, 4, 5, 6], name='inputs_poses_patches'\n",
        "        )\n",
        "        inputs_poses_shape = inputs_poses_patches.get_shape().as_list()\n",
        "        inputs_poses_patches = tf.reshape(inputs_poses_patches, [\n",
        "                                -1, shape[0]*shape[1]*shape[2], inputs_poses_shape[-1]\n",
        "                                ])\n",
        "\n",
        "        i_activations_patches = tf.transpose(\n",
        "          tf.gather(\n",
        "            tf.gather(\n",
        "              i_activations, hk_offsets, axis=1, name='gather_activations_height_kernel'\n",
        "            ), wk_offsets, axis=3, name='gather_activations_width_kernel'\n",
        "          ), perm=[0, 1, 3, 2, 4, 5], name='inputs_activations_patches'\n",
        "        )\n",
        "        i_activations_patches = tf.reshape(i_activations_patches, [\n",
        "                                -1, shape[0]*shape[1]*shape[2]]\n",
        "                                )\n",
        "        u_hat_vecs = vec_transformationByConv(\n",
        "                  inputs_poses_patches,\n",
        "                  inputs_poses_shape[-1], shape[0]*shape[1]*shape[2],\n",
        "                  inputs_poses_shape[-1], shape[3],\n",
        "                  )  \n",
        "        beta_a = _get_weights_wrapper(\n",
        "                name='beta_a', shape=[1, shape[3]]\n",
        "                )\n",
        "        poses, activations = routing(u_hat_vecs, beta_a, iterations, shape[3], i_activations_patches)\n",
        "        poses = tf.reshape(poses, [\n",
        "                    inputs_poses_shape[0], inputs_poses_shape[1],\n",
        "                    inputs_poses_shape[2], shape[3],\n",
        "                    inputs_poses_shape[-1]]\n",
        "                ) \n",
        "        activations = tf.reshape(activations, [\n",
        "                    inputs_poses_shape[0],inputs_poses_shape[1],\n",
        "                    inputs_poses_shape[2],shape[3]]\n",
        "                ) \n",
        "        nets = poses, activations            \n",
        "    return nets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "mNNBypTQPmMD"
      },
      "outputs": [],
      "source": [
        "def capsule_model_A(X, num_classes):\n",
        "    with tf.variable_scope('capsule_'+str(3),reuse=tf.AUTO_REUSE ):   \n",
        "        nets = _conv2d_wrapper(\n",
        "                X, shape=[3, 300, 1, 32], strides=[1, 2, 1, 1], padding='VALID', \n",
        "                add_bias=True, activation_fn=tf.nn.relu, name='conv1'\n",
        "            )\n",
        "        nets = capsules_init(nets, shape=[1, 1, 32, 16], strides=[1, 1, 1, 1], \n",
        "                             padding='VALID', pose_shape=16, add_bias=True, name='primary')                        \n",
        "        nets = capsule_conv_layer(nets, shape=[3, 1, 16, 16], strides=[1, 1, 1, 1], iterations=3, name='conv2')\n",
        "        nets = capsule_flatten(nets)\n",
        "        poses, activations = capsule_fc_layer(nets, num_classes, 3, 'fc2') \n",
        "    return poses, activations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "yXFlL2dFPqGd"
      },
      "outputs": [],
      "source": [
        "def capsule_model_A(X, num_classes):\n",
        "    with tf.variable_scope('capsule_'+str(3),reuse=tf.AUTO_REUSE ):   \n",
        "        nets = _conv2d_wrapper(\n",
        "                X, shape=[3, 300, 1, 32], strides=[1, 2, 1, 1], padding='VALID', \n",
        "                add_bias=True, activation_fn=tf.nn.relu, name='conv1'\n",
        "            )\n",
        "        nets = capsules_init(nets, shape=[1, 1, 32, 16], strides=[1, 1, 1, 1], \n",
        "                             padding='VALID', pose_shape=16, add_bias=True, name='primary')                        \n",
        "        nets = capsule_conv_layer(nets, shape=[3, 1, 16, 16], strides=[1, 1, 1, 1], iterations=3, name='conv2')\n",
        "        nets = capsule_flatten(nets)\n",
        "        poses, activations = capsule_fc_layer(nets, num_classes, 3, 'fc2') \n",
        "    return poses, activations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "3gRrOB-ZPw_y"
      },
      "outputs": [],
      "source": [
        "def capsule_model_B(X, num_classes):\n",
        "    poses_list = []\n",
        "    for _, ngram in enumerate([3,4,5]):\n",
        "        with tf.variable_scope('capsule_'+str(ngram),reuse=tf.AUTO_REUSE): \n",
        "            nets = _conv2d_wrapper(\n",
        "                X, shape=[ngram, 300, 1, 32], strides=[1, 2, 1, 1], padding='VALID', \n",
        "                add_bias=True, activation_fn=tf.nn.relu, name='conv1'\n",
        "            )\n",
        "            nets = capsules_init(nets, shape=[1, 1, 32, 16], strides=[1, 1, 1, 1], \n",
        "                                 padding='VALID', pose_shape=16, add_bias=True, name='primary')                        \n",
        "            nets = capsule_conv_layer(nets, shape=[3, 1, 16, 16], strides=[1, 1, 1, 1], iterations=3, name='conv2')\n",
        "            nets = capsule_flatten(nets)\n",
        "            poses, activations = capsule_fc_layer(nets, num_classes, 3, 'fc2')\n",
        "            poses_list.append(poses)\n",
        "    \n",
        "    poses = tf.reduce_mean(tf.convert_to_tensor(poses_list), axis=0) \n",
        "    activations = K.sqrt(K.sum(K.square(poses), 2))\n",
        "    return poses, activations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OVbhGp_iP4Rb"
      },
      "source": [
        "Loss functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "wYOZhjq3P2c6"
      },
      "outputs": [],
      "source": [
        "def spread_loss(labels, activations, margin):\n",
        "    activations_shape = activations.get_shape().as_list()\n",
        "    mask_t = tf.equal(labels, 1)\n",
        "    mask_i = tf.equal(labels, 0)    \n",
        "    activations_t = tf.reshape(\n",
        "      tf.boolean_mask(activations, mask_t), [activations_shape[0], 1]\n",
        "    )    \n",
        "    activations_i = tf.reshape(\n",
        "      tf.boolean_mask(activations, mask_i), [activations_shape[0], activations_shape[1] - 1]\n",
        "    )    \n",
        "    gap_mit = tf.reduce_sum(tf.square(tf.nn.relu(margin - (activations_t - activations_i))))\n",
        "    return gap_mit   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "gs2XxBwRP9G_"
      },
      "outputs": [],
      "source": [
        "def cross_entropy(y, preds):    \n",
        "    y = tf.argmax(y, axis=1)\n",
        "    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=preds, labels=y)                                               \n",
        "    loss = tf.reduce_mean(loss) \n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "V7-XOylvQAPE"
      },
      "outputs": [],
      "source": [
        "def margin_loss(y, preds):    \n",
        "    y = tf.cast(y,tf.float32)\n",
        "    loss = y * tf.square(tf.maximum(0., 0.9 - preds)) + \\\n",
        "        0.25 * (1.0 - y) * tf.square(tf.maximum(0., preds - 0.1))\n",
        "    loss = tf.reduce_mean(tf.reduce_sum(loss, axis=1))\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_vAotx0XQEv7"
      },
      "source": [
        "Training process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "zli7uICkQFrj"
      },
      "outputs": [],
      "source": [
        "class Args:\n",
        "  embedding_type = \"static\"\n",
        "  dataset = \"/content/modifiedDataPath.csv\"\n",
        "  loss_type = \"margin_loss\"\n",
        "  model_type = \"capsule-B\"\n",
        "  has_test = 1\n",
        "  has_dev = 1\n",
        "  num_epochs = 10\n",
        "  batch_size = 64\n",
        "  use_orphan = False\n",
        "  use_leaky = False\n",
        "  learning_rate = 0.001\n",
        "  margin = 0.2\n",
        "  num_classes = 5\n",
        "  vocab_size = vocab_size\n",
        "  vec_size = 300\n",
        "  max_sent = max_length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "wWQI4_VWQwLc"
      },
      "outputs": [],
      "source": [
        "args = Args()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "1MfbKb6sQ0tO"
      },
      "outputs": [],
      "source": [
        "with tf.device('/cpu:0'):\n",
        "    global_step = tf.train.get_or_create_global_step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "qhYith_XQ2UQ"
      },
      "outputs": [],
      "source": [
        "class BatchGenerator(object):\n",
        "    \"\"\"Generate and hold batches.\"\"\"\n",
        "    def __init__(self, dataset,label, batch_size,input_size, is_shuffle=True):\n",
        "      self._dataset = dataset\n",
        "      self._label = label\n",
        "      self._batch_size = batch_size    \n",
        "      self._cursor = 0      \n",
        "      self._input_size = input_size      \n",
        "      \n",
        "      if is_shuffle:\n",
        "          index = np.arange(len(self._dataset))\n",
        "          np.random.shuffle(index)\n",
        "          self._dataset = np.array(self._dataset)[index]\n",
        "          self._label = np.array(self._label)[index]\n",
        "      else:\n",
        "          self._dataset = np.array(self._dataset)\n",
        "          self._label = np.array(self._label)\n",
        "    def next(self):\n",
        "      if self._cursor + self._batch_size > len(self._dataset):\n",
        "          self._cursor = 0\n",
        "      \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"      \n",
        "      batch_x = self._dataset[self._cursor : self._cursor + self._batch_size,:]\n",
        "      batch_y = self._label[self._cursor : self._cursor + self._batch_size]\n",
        "      self._cursor += self._batch_size\n",
        "      return batch_x, batch_y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "QeEWCaowRFxx"
      },
      "outputs": [],
      "source": [
        "best_model = None\n",
        "best_epoch = 0\n",
        "best_acc_val = 0."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "h9D2PAtyRJjU"
      },
      "outputs": [],
      "source": [
        "lr = args.learning_rate\n",
        "m = args.margin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "MAvC-EF9RTDs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "02bfc9c3-f62b-4f1a-e93b-5b4b438067dc"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-60-8b1afb72241d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m   \u001b[0;31m#  w2v = np.array(embedding_vectors,dtype=np.float32)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0mW1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw2v\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m     \u001b[0mX_embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_lookup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0mX_embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_embedding\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'w2v' is not defined"
          ]
        }
      ],
      "source": [
        "acc_per_fold = []\n",
        "precision_per_fold = []\n",
        "recall_per_fold = []\n",
        "f1_per_fold = []\n",
        "\n",
        "kfold = KFold(n_splits=10, shuffle=True)\n",
        "\n",
        "fold_no = 1\n",
        "inputs = padded_docs\n",
        "targets = comment_labels\n",
        "\n",
        "for train, test in kfold.split(inputs, targets):\n",
        "  \n",
        "\n",
        "    n_iterations_per_epoch = len(inputs[train]) // args.batch_size\n",
        "    n_iterations_test = len(inputs[test]) // args.batch_size\n",
        "\n",
        "    mr_train1 = BatchGenerator(inputs[train], targets[train], args.batch_size, 0)    \n",
        "    mr_test1 = BatchGenerator(inputs[test], targets[test], args.batch_size, 0, is_shuffle=False)\n",
        "    best_accuracy = 0.\n",
        "    best_precision = 0.\n",
        "    best_recall = 0.\n",
        "    best_f1 = 0.\n",
        "\n",
        "    X = tf.placeholder(tf.int32, [args.batch_size, args.max_sent], name=\"input_x\")\n",
        "    y = tf.placeholder(tf.int64, [args.batch_size, args.num_classes], name=\"input_y\")\n",
        "    is_training = tf.placeholder_with_default(False, shape=())    \n",
        "    learning_rate = tf.placeholder(dtype='float32') \n",
        "    margin = tf.placeholder(shape=(),dtype='float32') \n",
        "\n",
        "    l2_loss = tf.constant(0.0)\n",
        "  #  w2v = np.array(embedding_vectors,dtype=np.float32)\n",
        "\n",
        "    W1 = tf.Variable(w2v, trainable = False)\n",
        "    X_embedding = tf.nn.embedding_lookup(W1, X)\n",
        "    X_embedding = X_embedding[...,tf.newaxis] \n",
        "\n",
        "\n",
        "    poses, activations = capsule_model_B(X_embedding, args.num_classes)\n",
        "    loss = margin_loss(y, activations) \n",
        "    y_pred = tf.argmax(activations, axis=1, name=\"y_proba\")    \n",
        "    correct = tf.equal(tf.argmax(y, axis=1), y_pred, name=\"correct\")\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
        "\n",
        "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate, name = 'opt'+str(fold_no))   \n",
        "    training_op = optimizer.minimize(loss, name=\"training_op\")\n",
        "    gradients, variables = zip(*optimizer.compute_gradients(loss)) \n",
        "    with tf.Session() as sess:\n",
        "\n",
        "      init = tf.global_variables_initializer()\n",
        "      sess.run(init)\n",
        "\n",
        "      for epoch in range(0,6): \n",
        "\n",
        "        for iteration in range(1, n_iterations_per_epoch + 1):\n",
        "            \n",
        "          \n",
        "          X_batch, y_batch = mr_train1.next()          \n",
        "          _, loss_train, probs, capsule_pose = sess.run(\n",
        "                [training_op, loss, activations, poses],\n",
        "                feed_dict={X: X_batch[:,:args.max_sent],\n",
        "                          y: y_batch,\n",
        "                          is_training: True,\n",
        "                          learning_rate:lr,\n",
        "                          margin:m})        \n",
        "          print(\"\\rIteration: {}/{} ({:.1f}%) epoch:{}  Loss: {:.5f}\".format(iteration, n_iterations_per_epoch, iteration * 100 / n_iterations_per_epoch, epoch+1, loss_train), end=\"\")                        \n",
        "        preds_list, y_list = [], []\n",
        "        for iteration in range(1, n_iterations_test + 1):\n",
        "          X_batch, y_batch = mr_test1.next()             \n",
        "          probs = sess.run([activations],\n",
        "                    feed_dict={X:X_batch[:,:args.max_sent],\n",
        "                                is_training: False})\n",
        "          preds_list = preds_list + probs[0].tolist()\n",
        "          y_list = y_list + y_batch.tolist()\n",
        "\n",
        "        y_list = np.array(y_list)\n",
        "        preds_probs = np.array(preds_list)  \n",
        "        labels = np.argmax(y_list, axis=1)\n",
        "        predictions = np.argmax(preds_probs, axis=1)\n",
        "\n",
        "        accuracy_fold = accuracy_score(labels, predictions)\n",
        "        precision_fold = precision_score(labels, predictions, average='weighted', zero_division = 0 )\n",
        "        recall_fold = recall_score(labels, predictions, average='weighted')\n",
        "        f1_fold = f1_score(labels, predictions, average='weighted')\n",
        "        if best_f1 <= f1_fold :\n",
        "          best_accuracy = accuracy_fold\n",
        "          best_precision = precision_fold\n",
        "          best_recall = recall_fold\n",
        "          best_f1 = f1_fold\n",
        "        \n",
        "\n",
        "      acc_per_fold.append(best_accuracy)\n",
        "      precision_per_fold.append(best_precision)\n",
        "      recall_per_fold.append(best_recall)\n",
        "      f1_per_fold.append(best_f1)\n",
        "      print(\"\\rFold: {} accuracy: {:.4f}%  Precision: {:.4f} recall: {:.4f} F1: {:.4f}\".format(fold_no, best_accuracy, best_precision, best_recall, best_f1))\n",
        "      if args.loss_type == 'margin_loss':    \n",
        "            m = min(0.9, m + 0.1)\n",
        "      fold_no += 1\n",
        "\n",
        "accuracy = np.mean(acc_per_fold)\n",
        "print('Accuracy: %f' % accuracy)\n",
        "# precision tp / (tp + fp)\n",
        "precision = np.mean(precision_per_fold)\n",
        "print('Precision: %f' % precision)\n",
        "# recall: tp / (tp + fn)\n",
        "recall = np.mean(recall_per_fold)\n",
        "print('Recall: %f' % recall)\n",
        "# f1: 2 tp / (2 tp + fp + fn)\n",
        "f1 = np.mean(f1_per_fold)\n",
        "print('F1 score: %f' % f1)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Untitled2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}